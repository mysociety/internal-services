#! /usr/bin/env python
#
# london:
# Screen scrape representatives from London assembly website
#
# Copyright (c) 2005 UK Citizens Online Democracy. All rights reserved.
# Email: jonathan@onegoodidea.com; WWW: http://www.mysociety.org/
#
# $Id: london,v 1.11 2013-05-07 13:09:02 dademcron Exp $

import re
from urllib import unquote
from urlparse import urljoin
from HTMLParser import HTMLParser
from cache import DiskCacheFetcher

LIST_PAGE = 'http://www.london.gov.uk/who-runs-london/the-london-assembly/members'

fetcher = DiskCacheFetcher('cache')

class HeadedTableParser( HTMLParser ):
    def __init__( self ):
        HTMLParser.__init__( self )
        self._state = 'CONTENT'
        self.list = {}
        self._data = ''
        self._url = None
    def handle_starttag( self, tag, attrs ):
        if self._state == 'CONTENT' and tag == 'h1':
            self._state = 'HEADER'
        elif self._state == 'HEADER' and tag == 'ul':
            self._state = 'LIST'
            self.list = []
        elif self._state == 'LIST' and tag == 'a':
            self._state = 'ITEM'
            self._data = ''
            self._url = [ value for key, value in attrs if key == 'href' ][0]
    def handle_endtag( self, tag ):
        if self._state == 'LIST' and tag == 'ul':
            self._state = 'CONTENT'
        elif self._state == 'ITEM' and tag == 'a':
            self._state = 'LIST'
            self.list.append( (self.tidy_data(), self._url ) )
    def handle_data( self, data ):
        self._data += data
    def handle_entityref( self, ref ):
        if ref == 'nbsp':
            self._data += ' '
        elif ref == 'rsquo':
            self._data += "'"
        elif ref == 'amp':
            self._data += '&'
        else:
            self._data += '&%s;' % ref
    def tidy_data( self ):
        return ' '.join( self._data.strip().split() )

class DetailPageParser( HTMLParser ):
    def __init__( self ):
        HTMLParser.__init__( self )
        self.email = None
    def handle_starttag( self, tag, attrs ):
        if tag == 'a' and len(attrs)==1 and attrs[0][0] == 'href':
            href = attrs[0][1]
            if href[0:7] == 'mailto:':
                self.email = href[7:]
    def handle_data( self, data ):
        m = re.match('\s*E-?mail: (.*)', data)
        if m:
            self.email = unquote(m.group(1)).strip().lower()


def splitName( name ):
    name = re.sub('^Dr ', '', name.replace('\xe2\x80\x99', "'"))
    names = name.split()
    return " ".join(names[1:]), names[0]

page = fetcher.fetch( LIST_PAGE )
parser = HeadedTableParser()
parser.feed( page )

print "First,Last,Constituency,Party,Email,Fax";

checks = { 'list': 0, 'constituency': 0 }

for data, url in parser.list:
    data = re.sub(' \(.*?\)', '', data)
    name, constituency, party = data.split(', ')
    surname, forename = splitName(name)
    if constituency == 'Londonwide':
        constituency = 'Proportionally Elected Member'
        checks['list'] += 1
    else:
        checks['constituency'] += 1
    url = url.replace('../../../', '../../') # Source has one too many
    url = urljoin( LIST_PAGE, url )
    page = fetcher.fetch( url )
    parser = DetailPageParser()
    parser.feed( page )
    email = parser.email
    print '"%s","%s","%s","%s","%s",""' % (forename, surname, constituency, party, email or '')
    
if checks['constituency'] != 14:
    raise StandardError, "Expected 14 constituency MEPs, got %d" % len(checks['constituency'])
if checks['list'] != 11:
    raise StandardError, "Expected 11 list MEPs, got %d" % len(checks['list'])

