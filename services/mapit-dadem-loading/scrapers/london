#! /usr/bin/env python
#
# london:
# Screen scrape representatives from London assembly website
#
# Copyright (c) 2005 UK Citizens Online Democracy. All rights reserved.
# Email: jonathan@onegoodidea.com; WWW: http://www.mysociety.org/
#
# $Id: london,v 1.11 2013-05-07 13:09:02 dademcron Exp $

import re
from urllib import unquote
from urlparse import urljoin
from HTMLParser import HTMLParser
from cache import DiskCacheFetcher

LIST_PAGE = 'https://www.london.gov.uk/people/assembly'

CONS_LOOKUP = {
    'Hounslow, Kingston upon Thames and Richmond upon Thames': 'South West',
    "Hackney, Islington and Waltham Forest": "North East",
    "Barking & Dagenham, City of London, Newham, Tower Hamlets": "City and East",
    "Hammersmith & Fulham, Kensington and Chelsea, City of Westminster": "West Central",
}

fetcher = DiskCacheFetcher('cache')

class HeadedTableParser( HTMLParser ):
    def __init__( self ):
        HTMLParser.__init__( self )
        self._state = 'CONTENT'
        self.list = []
        self._data = ''

    def state(self, state):
        self._state = state
        self._data = ''

    def handle_starttag( self, tag, attrs ):
        if self._state == 'CONTENT' and tag == 'article':
            self.state('ITEM')
            self._item = {}
        elif self._state == 'ITEM' and tag == 'a':
            self._item['url'] = [ value for key, value in attrs if key == 'href' ][0]
        elif self._state == 'ITEM' and tag == 'h1':
            self.state('NAME')
        elif self._state == 'POSTH1' and tag == 'div':
            attrs = dict(attrs)
            if 'class' in attrs and 'field--name-field-constituency-select' in attrs['class']:
                self._item['party'] = self.tidy_data()
                self.state('CONS')

    def handle_endtag( self, tag ):
        if self._state == 'NAME' and tag == 'h1':
            self._item['name'] = self.tidy_data()
            self.state('POSTH1')
        elif self._state == 'CONS' and tag == 'div':
            self._item['constituency'] = self.tidy_data()
            self.state('ITEM')
        elif self._state == 'ITEM' and tag == 'article':
            self.list.append(self._item)
            self.state('CONTENT')

    def handle_data( self, data ):
        self._data += data

    def handle_charref( self, ref ):
        if int(ref) == 39:
            self._data += "'"
        else:
            self._data += '&#%s;' % ref

    def handle_entityref( self, ref ):
        if ref == 'nbsp':
            self._data += ' '
        elif ref == 'rsquo':
            self._data += "'"
        elif ref == 'amp':
            self._data += '&'
        else:
            self._data += '&%s;' % ref

    def tidy_data( self ):
        return ' '.join( self._data.strip().split() )


class DetailPageParser( HTMLParser ):
    def __init__( self ):
        HTMLParser.__init__( self )
        self.email = ""
        self._state = 'NONE'

    def handle_starttag( self, tag, attrs ):
        if tag == 'li' and attrs == [('class', 'social-email')]:
            self._state = 'EMAIL'

    def handle_endtag(self, tag):
        if self._state == 'EMAIL' and tag == 'li':
            self._state = 'NONE'

    def handle_charref( self, ref ):
        if self._state != 'EMAIL': return
        if int(ref) == 39:
            self.email += "'"

    def handle_entityref( self, ref ):
        if self._state != 'EMAIL': return
        if ref == 'rsquo':
            self.email += "'"

    def handle_data( self, data ):
        if self._state == 'EMAIL':
            self.email += data.strip().lower()


def splitName( name ):
    name = re.sub('^Dr ', '', name.replace('\u2019', "'"))
    names = name.split()
    return " ".join(names[1:]), names[0]

page = fetcher.fetch( LIST_PAGE )
parser = HeadedTableParser()
parser.feed( page )

print "First,Last,Constituency,Party,Email,Fax";

checks = { 'list': 0, 'constituency': 0 }

for data in parser.list:
    surname, forename = splitName(data['name'])
    if data['constituency'] == 'Londonwide':
        constituency = 'Proportionally Elected Member'
        checks['list'] += 1
    else:
        constituency = CONS_LOOKUP.get(data['constituency'], data['constituency'])
        checks['constituency'] += 1
    url = urljoin( LIST_PAGE, data['url'] )
    page = fetcher.fetch( url )
    parser = DetailPageParser()
    parser.feed( page )
    email = parser.email
    print '"%s","%s","%s","%s","%s",""' % (forename, surname, constituency, data['party'], email or '')
    
if checks['constituency'] != 14:
    raise StandardError, "Expected 14 constituency MEPs, got %d" % len(checks['constituency'])
if checks['list'] != 11:
    raise StandardError, "Expected 11 list MEPs, got %d" % len(checks['list'])

